{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of the Process\n",
    "\n",
    "1. Build (two) embedding models to represent both items (games) and users (steam users) as an embedding vector.\n",
    "2. Find the similarity between game and user vectors\n",
    "    - Use distance metrics (cosine similarity, euclidean distance, etc.) to determine their similarity score\n",
    "3. Rank games by their similarity score with the user\n",
    "4. Remove games already owned in their library\n",
    "5. Present top 6 games from the remaining list of ranked games.\n",
    "\n",
    "## Benefits (Business worth)\n",
    "\n",
    "This method of finding recommendations overcomes the so-called \"Cold Start Problem\": it is hard to predict a user's interests when they are new users. By learning the embeddings of existing users and items, the system can make initial recommenations for new users or provide recommendations for new items based on their similarity to existing items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations for Modelling\n",
    "\n",
    "There are many types of recommender systems \n",
    "\n",
    "\n",
    "### Types of Models\n",
    "\n",
    "\n",
    "#### 1. Simple KNN\n",
    "\n",
    "#### 2. Item-Item Filtering\n",
    "\n",
    "#### 3. Item-User Filtering\n",
    "\n",
    "#### 4. Others?\n",
    "\n",
    "#### 5. NN\n",
    "\n",
    "### Accuracy Concerns\n",
    "\n",
    "The model's accuracy can be a valuable metric to determine if the recommendations being given to a user are appropriate.  An important consideration, however, is our tolerance for inaccurate recommendations making it into our top recommended list.  For instance, how much should we care if the system provides 5 good recommendations but 1 very bad recommendation compared to 6 mediocre recommendations?  In other words, how important is the system's ability to prevent false positives (i.e., its precision)?\n",
    "\n",
    "For my recommender system, I will give high preference to precision over accuracy since I want to prevent false positives whenever possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_df = pd.read_csv('data/clean_game_data.csv')\n",
    "users_df = pd.read_csv('data/clean_user_data.csv')\n",
    "recs_df = pd.read_csv('data/clean_recommendations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BSCapstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
